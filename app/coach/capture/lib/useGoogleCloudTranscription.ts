"use client";

import { useCallback, useEffect, useRef, useState } from "react";

interface TranscriptionState {
  transcript: string;
  interimTranscript: string;
  isListening: boolean;
  isConnected: boolean;
  error: string | null;
  confidence: number;
  micLevel: number;
  screenLevel: number;
  isMicrophoneEnabled: boolean;
}

export const useGoogleCloudTranscription = () => {
  const [state, setState] = useState<TranscriptionState>({
    transcript: "",
    interimTranscript: "",
    isListening: false,
    isConnected: false,
    error: null,
    confidence: 0,
    micLevel: 0,
    screenLevel: 0,
    isMicrophoneEnabled: false,
  });

  // ReferÃªncias para recursos de Ã¡udio
  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const micStreamRef = useRef<MediaStream | null>(null);
  const screenStreamRef = useRef<MediaStream | null>(null);
  const combinedStreamRef = useRef<MediaStream | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const micAnalyserRef = useRef<AnalyserNode | null>(null);
  const screenAnalyserRef = useRef<AnalyserNode | null>(null);
  const micGainRef = useRef<GainNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  // Conectar ao servidor WebSocket
  const connectWebSocket = useCallback(() => {
    try {
      const ws = new WebSocket('ws://localhost:8080');
      
      ws.onopen = () => {
        console.log('ðŸ”— Conectado ao servidor Speech-to-Text');
        setState(prev => ({ ...prev, isConnected: true, error: null }));
      };

      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          
          switch (data.type) {
            case 'transcript':
              setState(prev => ({
                ...prev,
                interimTranscript: data.isFinal ? '' : data.transcript,
                transcript: data.isFinal 
                  ? prev.transcript + ' ' + data.transcript 
                  : prev.transcript,
                confidence: data.confidence || 0,
              }));
              break;
              
            case 'interim':
              console.log('ðŸ“ TranscriÃ§Ã£o interim:', data.transcript);
              setState(prev => ({
                ...prev,
                interimTranscript: data.transcript,
                confidence: data.confidence || 0,
              }));
              break;
              
            case 'final':
              console.log('âœ… TranscriÃ§Ã£o final:', data.transcript);
              setState(prev => ({
                ...prev,
                transcript: prev.transcript + ' ' + data.transcript,
                interimTranscript: '',
                confidence: data.confidence || 0,
              }));
              break;
              
            case 'started':
              console.log('â–¶ï¸ TranscriÃ§Ã£o confirmada pelo servidor');
              break;
              
            case 'stopped':
              console.log('â¹ï¸ TranscriÃ§Ã£o parada pelo servidor');
              break;
              
            case 'error':
              setState(prev => ({ 
                ...prev, 
                error: data.message,
                isListening: false 
              }));
              break;
              
            case 'connected':
              console.log('âœ… Servidor confirmou conexÃ£o');
              break;

            case 'force-finalize-started':
              console.log('ðŸ§  FinalizaÃ§Ã£o forÃ§ada iniciada pelo servidor');
              break;

            case 'force-finalize-completed':
              console.log('âœ… FinalizaÃ§Ã£o forÃ§ada concluÃ­da, stream reiniciado');
              break;

            case 'force-finalize-error':
              console.warn('âš ï¸ Erro na finalizaÃ§Ã£o forÃ§ada:', data.message);
              break;
          }
        } catch (error) {
          console.error('âŒ Erro ao processar mensagem WebSocket:', error);
        }
      };

      ws.onerror = (error) => {
        console.error('âŒ Erro WebSocket:', error);
        setState(prev => ({ 
          ...prev, 
          error: 'Erro na conexÃ£o WebSocket',
          isConnected: false 
        }));
      };

      ws.onclose = () => {
        console.log('ðŸ”Œ ConexÃ£o WebSocket fechada');
        setState(prev => ({ 
          ...prev, 
          isConnected: false, 
          isListening: false 
        }));
      };

      wsRef.current = ws;
    } catch (error) {
      console.error('âŒ Erro ao conectar WebSocket:', error);
      setState(prev => ({ 
        ...prev, 
        error: 'Falha ao conectar ao servidor' 
      }));
    }
  }, []);

  // FunÃ§Ã£o para toggle do microfone
  const toggleMicrophoneCapture = useCallback((enabled: boolean) => {
    setState(prev => ({ ...prev, isMicrophoneEnabled: enabled }));
    
    // Controlar o gain do microfone
    if (micGainRef.current) {
      micGainRef.current.gain.value = enabled ? 1 : 0;
      console.log(`ðŸŽ™ï¸ Microfone ${enabled ? 'HABILITADO' : 'DESABILITADO'}`);
    }
  }, []);

  // FunÃ§Ã£o para calcular nÃ­vel de Ã¡udio
  const calculateAudioLevel = useCallback((analyser: AnalyserNode): number => {
    const dataArray = new Uint8Array(analyser.frequencyBinCount);
    analyser.getByteFrequencyData(dataArray);
    
    const sum = dataArray.reduce((acc, val) => acc + val, 0);
    return (sum / dataArray.length) / 255;
  }, []);

  // Atualizar nÃ­veis de Ã¡udio em tempo real
  const updateAudioLevels = useCallback(() => {
    if (micAnalyserRef.current && screenAnalyserRef.current) {
      const micLevel = state.isMicrophoneEnabled ? calculateAudioLevel(micAnalyserRef.current) : 0;
      const screenLevel = calculateAudioLevel(screenAnalyserRef.current);
      
      setState(prev => ({ ...prev, micLevel, screenLevel }));
    }
    
    animationFrameRef.current = requestAnimationFrame(updateAudioLevels);
  }, [calculateAudioLevel, state.isMicrophoneEnabled]);

  // Configurar captura de Ã¡udio
  const setupAudioCapture = useCallback(async () => {
    try {
      // Capturar microfone
      const micStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000,
        }
      });

      // Capturar tela com Ã¡udio
      const screenStream = await navigator.mediaDevices.getDisplayMedia({
        video: true,
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          sampleRate: 16000,
        }
      });

      micStreamRef.current = micStream;
      screenStreamRef.current = screenStream;

      // Criar contexto de Ã¡udio
      const audioContext = new AudioContext({ sampleRate: 16000 });
      audioContextRef.current = audioContext;

      // Criar nÃ³s de Ã¡udio
      const micSource = audioContext.createMediaStreamSource(micStream);
      const screenSource = audioContext.createMediaStreamSource(screenStream);
      const destination = audioContext.createMediaStreamDestination();

      // Criar gain node para controlar o microfone
      const micGain = audioContext.createGain();
      micGain.gain.value = state.isMicrophoneEnabled ? 1 : 0; // Iniciar conforme estado
      micGainRef.current = micGain;

      // Criar analisadores para monitoramento
      const micAnalyser = audioContext.createAnalyser();
      const screenAnalyser = audioContext.createAnalyser();
      micAnalyser.fftSize = 2048;
      screenAnalyser.fftSize = 2048;

      micAnalyserRef.current = micAnalyser;
      screenAnalyserRef.current = screenAnalyser;

      // Conectar e processar Ã¡udio com controle do microfone
      micSource.connect(micAnalyser);
      micSource.connect(micGain); // Microfone passa pelo gain
      micGain.connect(destination); // Gain controlado vai para o destino
      
      screenSource.connect(screenAnalyser);
      screenSource.connect(destination); // Tela vai direto para o destino

      combinedStreamRef.current = destination.stream;

      // Criar processador para enviar Ã¡udio ao servidor
      const processor = audioContext.createScriptProcessor(4096, 1, 1);
      processorRef.current = processor;

      // Conectar o stream combinado ao processador
      const combinedSource = audioContext.createMediaStreamSource(destination.stream);
      combinedSource.connect(processor);
      processor.connect(audioContext.destination);

      processor.onaudioprocess = (event) => {
        if (wsRef.current?.readyState === WebSocket.OPEN) {
          const inputData = event.inputBuffer.getChannelData(0);
          
          // Converter para 16-bit PCM
          const pcm16 = new Int16Array(inputData.length);
          for (let i = 0; i < inputData.length; i++) {
            pcm16[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32767));
          }
          
          // Enviar Ã¡udio como base64
          const uint8Array = new Uint8Array(pcm16.buffer);
          const audioBase64 = btoa(String.fromCharCode.apply(null, Array.from(uint8Array)));
          wsRef.current.send(JSON.stringify({
            type: 'audio',
            audio: audioBase64
          }));
        }
      };

      // Iniciar monitoramento de nÃ­veis
      updateAudioLevels();

      console.log('ðŸŽ™ï¸ Captura de Ã¡udio configurada com sucesso');
      console.log(`ðŸŽ™ï¸ Microfone ${state.isMicrophoneEnabled ? 'HABILITADO' : 'DESABILITADO'}`);
      return true;
    } catch (error) {
      console.error('âŒ Erro ao configurar captura de Ã¡udio:', error);
      setState(prev => ({ 
        ...prev, 
        error: 'Erro ao acessar microfone ou tela' 
      }));
      return false;
    }
  }, [updateAudioLevels, state.isMicrophoneEnabled]);

  // Iniciar transcriÃ§Ã£o
  const startListening = useCallback(async () => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      setState(prev => ({ 
        ...prev, 
        error: 'Servidor nÃ£o conectado' 
      }));
      return;
    }

    try {
      setState(prev => ({ ...prev, error: null }));
      
      const audioSetupSuccess = await setupAudioCapture();
      if (!audioSetupSuccess) return;

      // Iniciar transcriÃ§Ã£o no servidor
      wsRef.current.send(JSON.stringify({ type: 'start' }));
      
      setState(prev => ({ ...prev, isListening: true }));
      console.log('ðŸš€ TranscriÃ§Ã£o iniciada');
    } catch (error) {
      console.error('âŒ Erro ao iniciar transcriÃ§Ã£o:', error);
      setState(prev => ({ 
        ...prev, 
        error: 'Erro ao iniciar transcriÃ§Ã£o',
        isListening: false 
      }));
    }
  }, [setupAudioCapture]);

  // Parar transcriÃ§Ã£o
  const stopListening = useCallback(() => {
    try {
      // Parar transcriÃ§Ã£o no servidor
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        wsRef.current.send(JSON.stringify({ type: 'stop' }));
      }

      // Parar animaÃ§Ã£o
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
      }

      // Limpar recursos de Ã¡udio
      if (processorRef.current) {
        processorRef.current.disconnect();
        processorRef.current = null;
      }

      if (micGainRef.current) {
        micGainRef.current.disconnect();
        micGainRef.current = null;
      }

      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }

      if (micStreamRef.current) {
        micStreamRef.current.getTracks().forEach(track => track.stop());
        micStreamRef.current = null;
      }

      if (screenStreamRef.current) {
        screenStreamRef.current.getTracks().forEach(track => track.stop());
        screenStreamRef.current = null;
      }

      setState(prev => ({ 
        ...prev, 
        isListening: false,
        micLevel: 0,
        screenLevel: 0 
      }));
      
      console.log('â¹ï¸ TranscriÃ§Ã£o parada');
    } catch (error) {
      console.error('âŒ Erro ao parar transcriÃ§Ã£o:', error);
    }
  }, []);

  // Limpar transcriÃ§Ã£o
  const clearTranscript = useCallback(() => {
    setState(prev => ({ 
      ...prev, 
      transcript: '', 
      interimTranscript: '' 
    }));
  }, []);

  // ForÃ§ar finalizaÃ§Ã£o de transcriÃ§Ãµes interim para anÃ¡lise
  const forceFinalize = useCallback(() => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.warn('âš ï¸ WebSocket nÃ£o conectado para forÃ§ar finalizaÃ§Ã£o');
      return;
    }
    
    if (!state.isListening) {
      console.warn('âš ï¸ TranscriÃ§Ã£o nÃ£o estÃ¡ ativa para forÃ§ar finalizaÃ§Ã£o');
      return;
    }

    console.log('ðŸ§  Enviando comando force-finalize para servidor');
    wsRef.current.send(JSON.stringify({ 
      type: 'force-finalize',
      reason: 'user-analysis' 
    }));
  }, [state.isListening]);

  // Conectar ao WebSocket na inicializaÃ§Ã£o
  useEffect(() => {
    connectWebSocket();
    
    return () => {
      if (wsRef.current) {
        wsRef.current.close();
      }
    };
  }, [connectWebSocket]);

  // Limpar recursos na desmontagem
  useEffect(() => {
    return () => {
      stopListening();
    };
  }, [stopListening]);

  return {
    transcript: state.transcript,
    interimTranscript: state.interimTranscript,
    isListening: state.isListening,
    isConnected: state.isConnected,
    error: state.error,
    confidence: state.confidence,
    micLevel: state.micLevel,
    screenLevel: state.screenLevel,
    isMicrophoneEnabled: state.isMicrophoneEnabled,
    startListening,
    stopListening,
    clearTranscript,
    connectWebSocket,
    toggleMicrophoneCapture,
    forceFinalize,
  };
}; 